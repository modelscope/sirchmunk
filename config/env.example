# ===== Sirchmunk Configuration =====
# Shared by Web API server and MCP server.
# Copy to ~/.sirchmunk/.env and edit.

# ===== LLM Settings =====
# LLM API base URL (OpenAI-compatible endpoint)
LLM_BASE_URL=https://api.openai.com/v1

# LLM API key (REQUIRED - get from your LLM provider)
LLM_API_KEY=

# LLM model name
LLM_MODEL_NAME=gpt-5.2

# LLM request timeout in seconds
LLM_TIMEOUT=60.0

# ===== Sirchmunk Settings =====
# Working directory for Sirchmunk data and cache
SIRCHMUNK_WORK_PATH=~/.sirchmunk

# Enable verbose logging (true/false)
SIRCHMUNK_VERBOSE=false

# ===== Search Default Settings =====
# Maximum directory depth to search
DEFAULT_MAX_DEPTH=5

# Number of top files to return
DEFAULT_TOP_K_FILES=3

# Number of keyword granularity levels (1-5)
DEFAULT_KEYWORD_LEVELS=3

# Timeout for grep operations in seconds
GREP_TIMEOUT=60.0

# ===== Cluster Settings =====
# Enable knowledge cluster reuse with embeddings (true/false)
SIRCHMUNK_ENABLE_CLUSTER_REUSE=true

# Similarity threshold for cluster reuse (0.0-1.0)
CLUSTER_SIM_THRESHOLD=0.85

# Number of similar clusters to retrieve (1-10)
CLUSTER_SIM_TOP_K=3

# Maximum number of queries to keep per cluster (FIFO strategy)
MAX_QUERIES_PER_CLUSTER=5

# ===== MCP Server Settings =====
# MCP server name
MCP_SERVER_NAME=sirchmunk

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
MCP_LOG_LEVEL=INFO

# MCP transport protocol (stdio or http)
MCP_TRANSPORT=stdio

# ===== HTTP Transport Settings (only used when MCP_TRANSPORT=http) =====
# Host for HTTP transport
MCP_HOST=localhost

# Port for HTTP transport
MCP_PORT=8080

# ===== Optional: Custom Embedding Model Settings =====
# Embedding model ID (from ModelScope or HuggingFace)
# EMBEDDING_MODEL_ID=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Embedding model cache directory
# EMBEDDING_CACHE_DIR=${SIRCHMUNK_WORK_PATH}/.cache/models
