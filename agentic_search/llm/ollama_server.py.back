import json
import sys
import time
from typing import List, Optional, Union

import httpx
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger
from pydantic import BaseModel, Field

# loguru configuration
logger.remove()  # Remove default handler
logger.add(
    "logs/ollama_proxy_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="7 days",
    level="DEBUG",
    enqueue=True,     # Async logging
)
logger.add(sys.stderr, level="INFO", colorize=True)

# â€”â€”â€”â€”â€”â€” Configuration â€”â€”â€”â€”â€”â€”
OLLAMA_BASE_URL = "http://localhost:11434"
DEFAULT_MODEL = "modelscope.cn/Qwen/Qwen3-0.6B-GGUF:latest"
DEFAULT_STREAM = True


# Initialize FastAPI app
app = FastAPI(
    title="Ollama OpenAI-Compatible API Proxy",
    description="Proxy Ollama models to OpenAI API format (streaming & non-streaming).",
)

"""
Usage:
1. Start Ollama server locally:
`uvicorn ollama_server:app --host 0.0.0.0 --port 8000 --app-dir /path/to/open-agentic-search/agentic_search/llm`       # noqa

2. Call the API using OpenAI-compatible clients: refer to the docstring in `ollama_stream_generator`.
"""

# TODO: support high-throughput settings !!


# â€”â€”â€”â€”â€”â€” Loguru Middleware â€”â€”â€”â€”â€”â€”
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """
    Log incoming HTTP requests and outgoing responses using loguru.

    Args:
        request: The incoming FastAPI request.
        call_next: Callable to proceed to the next middleware/handler.

    Returns:
        The response after processing.
    """
    start_time = time.time()

    # Log request
    logger.info(
        f"â†’ {request.method} {request.url.path} | client: {request.client.host}"
    )

    try:
        response: Response = await call_next(request)
        process_time = (time.time() - start_time) * 1000  # ms
        logger.info(
            f"â† {response.status_code} {request.method} {request.url.path} "
            f"| {process_time:.1f}ms"
        )
        return response
    except Exception as e:
        process_time = (time.time() - start_time) * 1000
        logger.error(
            f"âœ— {request.method} {request.url.path} | {e.__class__.__name__}: {e} "
            f"| {process_time:.1f}ms"
        )
        raise


# â€”â€”â€”â€”â€”â€” Pydantic Models â€”â€”â€”â€”â€”â€”
class Message(BaseModel):
    """Represents a message in a chat conversation."""

    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    """Request schema for /v1/chat/completions endpoint, matching OpenAI API."""

    model: str = Field(
        default=DEFAULT_MODEL, description="The model to use for completion."
    )
    messages: List[Message] = Field(
        ..., description="A list of messages comprising the conversation."
    )
    temperature: Optional[float] = Field(
        default=0.7, ge=0.0, le=2.0, description="Sampling temperature."
    )
    top_p: Optional[float] = Field(
        default=0.9, ge=0.0, le=1.0, description="Nucleus sampling parameter."
    )
    max_tokens: Optional[int] = Field(
        default=None,
        description="Maximum number of tokens to generate. Ollama uses 'num_predict'.",
    )
    stream: Optional[bool] = Field(
        default=DEFAULT_STREAM, description="Whether to stream the response."
    )
    stop: Optional[Union[str, List[str]]] = Field(
        default=None, description="Stop sequences (string or list)."
    )
    frequency_penalty: Optional[float] = Field(
        default=0.0, ge=-2.0, le=2.0, description="Frequency penalty."
    )
    presence_penalty: Optional[float] = Field(
        default=0.0, ge=-2.0, le=2.0, description="Presence penalty."
    )
    seed: Optional[int] = Field(
        default=None,
        description="Random seed for reproducibility (Ollama supports this).",
    )


# â€”â€”â€”â€”â€”â€” Helper Functions â€”â€”â€”â€”â€”â€”
async def call_ollama_non_stream(payload: dict) -> dict:
    """
    Call Ollama's /api/chat endpoint in non-streaming mode.

    Args:
        payload: JSON payload conforming to Ollama's chat API.

    Returns:
        Full response JSON from Ollama.

    Raises:
        HTTPException: If Ollama returns non-200 status.
    """
    async with httpx.AsyncClient(timeout=300.0) as client:
        resp = await client.post(f"{OLLAMA_BASE_URL}/api/chat", json=payload)
        if resp.status_code != 200:
            error_detail = f"Ollama returned {resp.status_code}: {resp.text}"
            logger.error(error_detail)
            raise HTTPException(status_code=resp.status_code, detail=error_detail)
        return resp.json()


async def ollama_stream_generator(request: ChatCompletionRequest):
    """
    Generator that streams Ollama's response and converts it to OpenAI SSE format.

    Strictly follows OpenAI's Server-Sent Events (SSE) specification:
      data: {json}\n\n
      data: [DONE]\n\n

    Args:
        request: The original ChatCompletionRequest.

    Yields:
        Strings of properly formatted SSE chunks.

    Examples:
        >>> from openai import OpenAI
        >>>
        >>> client = OpenAI(base_url="http://localhost:8000/v1", api_key="ollama")
        >>> model = 'modelscope.cn/Qwen/Qwen3-0.6B-GGUF'  # Ollama model name, not URL
        >>>
        >>> stream = client.chat.completions.create(
        ...     model=model,
        ...     messages=[{"role": "user", "content": "Who are you?"}],
        ...     stream=True,
        ...     temperature=0.3
        ... )
        >>>
        >>> print("ðŸ¤– Assistant: ", end="", flush=True)
        >>> for i, chunk in enumerate(stream):
        ...     delta = chunk.choices[0].delta
        ...     if delta.role:
        ...         print(f"[role={delta.role}] ", end="", flush=True)
        ...     if delta.content:
        ...         print(delta.content, end="", flush=True)
        >>> print("\\nâœ… Stream finished.")
    """
    payload = {
        "model": request.model,
        "messages": [{"role": m.role, "content": m.content} for m in request.messages],
        "stream": True,
        "options": {
            "temperature": request.temperature,
            "top_p": request.top_p,
            "num_predict": request.max_tokens or -1,
            "stop": request.stop or [],
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
        },
    }
    if request.seed is not None:
        payload["options"]["seed"] = request.seed

    created_ts = int(time.time())
    chunk_id = f"chatcmpl-{created_ts}-{hash(request.model) % 10000:04}"

    try:
        # ðŸŽ¯ First chunk: send role=assistant to initialize
        first_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created_ts,
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "delta": {"role": "assistant"},
                    "logprobs": None,
                    "finish_reason": None,
                }
            ],
        }
        yield f"data: {json.dumps(first_chunk, ensure_ascii=False)}\n\n"

        async with httpx.AsyncClient(timeout=300.0) as client:
            async with client.stream(
                "POST", f"{OLLAMA_BASE_URL}/api/chat", json=payload
            ) as resp:
                if resp.status_code != 200:
                    error_text = await resp.aread()
                    raise HTTPException(
                        status_code=resp.status_code, detail=error_text.decode()
                    )

                async for line in resp.aiter_lines():
                    if line.strip():
                        try:
                            ollama_chunk = json.loads(line)
                            content = ollama_chunk.get("message", {}).get("content", "")
                            if not content and not ollama_chunk.get("done", False):
                                continue  # Skip empty non-final chunks

                            # Build delta chunk
                            delta = {"content": content} if content else {}

                            openai_chunk = {
                                "id": chunk_id,
                                "object": "chat.completion.chunk",
                                "created": created_ts,
                                "model": request.model,
                                "choices": [
                                    {
                                        "index": 0,
                                        "delta": delta,
                                        "logprobs": None,  # Required by openai-py
                                        "finish_reason": None,
                                    }
                                ],
                            }

                            if ollama_chunk.get("done", False):
                                openai_chunk["choices"][0]["finish_reason"] = "stop"

                            # âœ… CRITICAL: prefix with "data: " and double \n
                            yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

                            if ollama_chunk.get("done", False):
                                break

                        except Exception as e:
                            logger.warning(f"Parse error in stream line: {e}")

        # ðŸŽ¯ Final [DONE] event â€” must be `data: [DONE]`
        yield "data: [DONE]\n\n"

    except httpx.RequestError as e:
        logger.error(f"Ollama stream connection error: {e}")
        raise HTTPException(status_code=500, detail=f"Stream error: {e}")


# â€”â€”â€”â€”â€”â€” API Endpoints â€”â€”â€”â€”â€”â€”
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completion endpoint.

    Supports both streaming (SSE) and non-streaming (JSON) responses.
    Delegates to Ollama backend.

    Args:
        request: Chat completion request.

    Returns:
        StreamingResponse (if stream=True) or JSONResponse (if stream=False).

    Raises:
        HTTPException: On backend errors.
    """
    logger.debug(
        f"Handling chat completion: model={request.model}, stream={request.stream}"
    )

    payload = {
        "model": request.model,
        "messages": [{"role": m.role, "content": m.content} for m in request.messages],
        "stream": request.stream,
        "options": {
            "temperature": request.temperature,
            "top_p": request.top_p,
            "num_predict": request.max_tokens or -1,
            "stop": request.stop or [],
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
        },
    }
    if request.seed is not None:
        payload["options"]["seed"] = request.seed

    if request.stream:
        return StreamingResponse(
            ollama_stream_generator(request), media_type="text/event-stream"
        )
    else:
        try:
            ollama_resp = await call_ollama_non_stream(payload)
            created = int(time.time())
            content = ollama_resp["message"]["content"]
            prompt_tokens = ollama_resp.get("prompt_eval_count", 0)
            completion_tokens = ollama_resp.get("eval_count", 0)

            openai_resp = {
                "id": f"chatcmpl-{created}",
                "object": "chat.completion",
                "created": created,
                "model": request.model,
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": content,
                        },
                        "finish_reason": "stop",
                    }
                ],
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens,
                },
            }
            return JSONResponse(openai_resp)

        except Exception as e:
            logger.exception("Non-stream completion failed")
            raise HTTPException(status_code=500, detail=f"Non-stream error: {e}")


@app.get("/health")
async def health():
    """
    Health check endpoint.

    Returns:
        JSON status object.
    """
    return {
        "status": "ok",
        "model": DEFAULT_MODEL,
        "ollama_base_url": OLLAMA_BASE_URL,
        "default_stream": DEFAULT_STREAM,
    }


# â€”â€”â€”â€”â€”â€” Optional: /v1/models endpoint â€”â€”â€”â€”â€”â€”
@app.get("/v1/models")
async def list_models():
    """
    List available models (simplified â€” only returns default model for now).

    Returns:
        OpenAI-compatible model list response.
    """
    return {
        "object": "list",
        "data": [
            {
                "id": DEFAULT_MODEL,
                "object": "model",
                "created": int(time.time()) - 86400,  # ~1 day ago
                "owned_by": "ollama",
            }
        ],
    }
